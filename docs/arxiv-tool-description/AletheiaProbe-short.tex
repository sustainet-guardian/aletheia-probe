\documentclass[fleqn,10pt]{SelfArx}

\usepackage[english]{babel}
\usepackage{listings}
\usepackage{xcolor}
%\usepackage{siunitx}
\usepackage{orcidlink}

\frenchspacing

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
  backgroundcolor=\color{backcolour},
  commentstyle=\color{codegreen},
  keywordstyle=\color{magenta},
  numberstyle=\tiny\color{codegray},
  stringstyle=\color{codepurple},
  basicstyle=\ttfamily\footnotesize,
  breakatwhitespace=false,
  breaklines=true,
  captionpos=b,
  keepspaces=true,
  numbers=left,
  numbersep=5pt,
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2
}

\lstset{style=mystyle}

\usepackage[style=numeric,sorting=none,backend=biber]{biblatex}
\addbibresource{AletheiaProbe-short.bib}
\usepackage{csquotes}
\usepackage{enumitem}

\setlength{\headheight}{20.96901pt}
\setlength{\columnsep}{0.55cm}
\setlength{\fboxrule}{0.75pt}

\definecolor{color1}{RGB}{0,0,90}
\definecolor{color2}{RGB}{0,20,20}

\newcommand{\ilcode}[1]{%
  \texttt{\small\textcolor{darkgray}{#1}}%
}

\usepackage{hyperref}
\hypersetup{
  hidelinks,
  colorlinks,
  breaklinks=true,
  urlcolor=color2,
  citecolor=color1,
  linkcolor=color1,
  bookmarksopen=false,
  pdftitle={Aletheia-Probe: Automated Journal Quality Assessment},
  pdfauthor={Andreas Florath}
}

\JournalInfo{V1}
\Archive{}
\PaperTitle{Aletheia-Probe: A Multi-Source Tool for Automated Journal Assessment}
\Authors{Andreas Florath\orcidlink{0009-0001-6471-7372}\textsuperscript{1}}
\affiliation{\textsuperscript{1}\textit{Deutsche Telekom AG, Andreas.Florath@telekom.de}}
\Keywords{Predatory Journals --- Academic Integrity --- Publication Ethics --- Journal Assessment --- Research Workflows}
\newcommand{\keywordname}{Keywords}

\Abstract{Researchers lack practical tools to efficiently assess
  journal legitimacy while conducting literature reviews and selecting
  publication venues. This paper introduces Aletheia-Probe, an
  automated assessment tool that aggregates data from multiple
  authoritative sources including DOAJ, Beall's List, OpenAlex,
  Crossref, and others. The tool combines curated databases with
  pattern analysis to provide assessments with confidence indicators
  covering over 240 million publication records. It offers
  command-line and programmatic interfaces for integration into
  research workflows, including batch processing of BibTeX files. We
  present the tool's architecture, design philosophy, and integration
  approach. This initial presentation establishes the system design
  and demonstrates practical applicability; comprehensive empirical
  validation across large journal datasets will be presented in
  forthcoming work.}

\begin{document}

\maketitle

\tableofcontents 

\section{Introduction}
Predatory academic journals pose a significant and growing threat to
scholarly integrity. Article volumes in predatory journals grew from
53,000 in 2010 to over 420,000 in 2014~\cite{shen2015predatory},
affecting researchers across all scientific disciplines---from
mathematics and engineering to nursing and biomedical
sciences~\cite{agricola2025,gabrielsson2020nursing}. These journals
increasingly mimic legitimate publications through
professional-looking websites and editorial structures, making them
difficult to distinguish when researchers choose publication venues or
conduct literature reviews.

Multiple organizations maintain lists of legitimate or predatory
journals --- Beall's List~\cite{bealls_list}, the Directory of Open
Access Journals (DOAJ)~\cite{doaj}, indexing services like
Scopus~\cite{scopus}, and regional ministry
lists~\cite{DGRSDT2024}. However, researchers lack practical tools to
use these scattered resources. Who has time to manually check hundreds
or even thousands of journal entries across multiple databases and
websites? A literature review with 150 papers requires verifying each
source journal, which conservatively amounts to 12 hours of tedious
manual work across multiple websites.

This paper introduces Aletheia-Probe~\cite{aletheia_probe}, an
automated journal assessment tool that combines data from multiple
trusted sources with transparent confidence indicators. ``Aletheia''
comes from ancient Greek philosophy, representing truth and
unconcealment --- reflecting the tool's mission to reveal the truth
about academic journals. By reducing assessment overhead from hours to
minutes, the tool transforms scattered quality indicators into
actionable guidance at the point of decision.

\textbf{Setting Realistic Expectations:} If you are an honest
researcher using established, well-known journals and conferences
through major search engines and databases, you will likely never
encounter predatory venues. This tool functions like a virus scanner
for academic publishing --- you should have it installed and running
in your everyday work, but hopefully never receive any warnings. It is
designed to catch edge cases and protect against less obvious threats
that might slip through normal research workflows.

\textbf{Scope of This Paper:} This work presents the architecture,
design principles, and practical implementation of Aletheia-Probe. We
describe the data aggregation approach, assessment methodology, and
integration capabilities that enable researchers to incorporate journal
quality checking into their workflows. The paper demonstrates the
tool's practical applicability through representative examples.
Comprehensive empirical validation, including systematic coverage
analysis across large journal datasets and comparative evaluation
against existing approaches, will be presented in forthcoming work.

\section{Problem and Motivation}

\textbf{Concrete Researcher Scenarios.} Consider a systematic review
author conducting a meta-analysis who must verify 300 journals for
PRISMA compliance~\cite{page2021prisma}. PRISMA requires explicit
documentation of source quality criteria. DOAJ alone doesn't cover
many regional journals, so the researcher must manually check each
journal against multiple databases --- DOAJ for open access journals,
Scopus for indexed venues, Beall's List archives for known predatory
publishers, ministry lists for regional warnings. Cross-referencing
300 journals across 5--10 sources, accounting for name variations and
ISSN formats, conservatively requires 12 hours of tedious, error-prone
manual work.

Or consider an early-career researcher seeking to publish in an
open-access journal. They receive invitations from dozens of venues
with professional-looking websites, impressive editorial boards, and
promises of fast peer review. Without institutional guidance, how do
they distinguish legitimate journals from predatory ones? Manually
checking each venue against multiple databases is time-consuming, and
missing a predatory journal can damage career prospects.

Similarly, research integrity officers investigating suspicious
citations must verify journal legitimacy across institutional
publications. Librarians managing budgets need to assess whether
open-access journals merit institutional support. In all these
scenarios, researchers need quick, reliable assessments but face
scattered information across incompatible sources.

\textbf{Compounding Factors.} The challenge intensifies due to several
factors. First, journal proliferation has accelerated with open access
publishing, creating a larger landscape to evaluate. Second, while
predatory journals do evolve tactics to mimic legitimate publications,
no automated tool can detect all novel approaches --- curated lists
are retrospective, and heuristic checks catch obvious anomalies but
not sophisticated mimicry. Assessment tools form one layer of defense
alongside institutional guidelines and domain expertise. Third,
effective assessment requires checking multiple independent sources,
as no single database provides comprehensive coverage. Finally,
researchers need answers in seconds, not hours --- publication venue
decisions cannot wait for days of manual verification.

Previous work has extensively studied predatory publishing. Bohannon's
sting operation submitted flawed papers to 304 open-access journals
and found over half accepted them with minimal peer
review~\cite{bohannon2013garbage}. Agricola et al. analyzed predatory
journal characteristics and provided practical
recommendations~\cite{agricola2025}. Grudniewicz et al. worked with
experts to standardize the definition of ``predatory
journal''~\cite{grudniewicz2019predatory}. While machine learning
approaches like AJPC have been explored~\cite{chen2023ajpc}, no
existing tool systematically aggregates authoritative curated sources
with transparent confidence indicators.

\section{System Design and Features}

\subsection{Core Design Principles}

Aletheia-Probe's architecture follows three fundamental principles that
shape all design decisions and distinguish it from other approaches:

\textbf{1. Aggregator, Not Creator.} The tool does not create its own
journal quality data or judgments. Instead, it systematically
aggregates information from existing authoritative sources maintained
by domain experts and established organizations (DOAJ, Beall's List,
Scopus, ministry lists, etc.). This principle acknowledges that journal
quality assessment requires domain expertise distributed across the
scholarly community. The tool's value lies in making scattered
authoritative information accessible and queryable, not in substituting
its own algorithmic judgments for expert human curation.

\textbf{2. Multiple Independent Sources.} The tool queries multiple
data sources for each assessment and reports all findings. No single
database provides comprehensive coverage or perfect accuracy. DOAJ
covers legitimate open access journals but not subscription venues.
Beall's List captures historically identified predatory publishers but
is no longer updated. Scopus indexes established journals but has
limited regional coverage. By consulting multiple independent sources,
the tool provides more complete information than any single database
and enables researchers to see when sources agree or conflict.

\textbf{3. Transparent Reasoning.} Every assessment explicitly reports
which sources were consulted, what each source found, and how that
evidence contributed to the overall classification. This transparency
is not a limitation but a core feature. Rather than providing opaque
``trust this score'' outputs, the tool shows its work, enabling
researchers to understand the basis for each assessment and make
informed decisions. When data is insufficient, the tool says so
explicitly (returning \texttt{UNKNOWN}) rather than manufacturing
low-confidence guesses. This honest reporting of uncertainty respects
researchers' need to understand the reliability of information they act
upon.

These principles prioritize transparency, expert curation, and
comprehensive information over algorithmic sophistication. The
following subsections describe how these principles manifest in system
architecture and functionality.

\subsection{Architecture Overview}
The system comprises five main components. The data synchronization
layer downloads information from multiple data-sets during initial
setup, organizing data for fast searching. The backend abstraction
layer provides a common interface to query all sources, whether local
databases or web APIs. The assessment dispatcher sends queries to all
relevant backends simultaneously, improving speed. The aggregation
component combines responses from multiple sources into a unified
assessment, reporting a confidence indicator alongside the
classification. Finally, command-line and library interfaces provide
two usage modes.

\subsection{Data Sources}

The tool integrates multiple data sources organized into two
categories:

\textbf{Curated Database Backends} provide authoritative yes/no
determinations for journals they cover through expert human curation.
The Directory of Open Access Journals (DOAJ) maintains a curated list
of over 22,000 legitimate open access journals~\cite{doaj}. Beall's
List, while no longer actively maintained, provides historical
archives of approximately 2,900 predatory publishers and
journals~\cite{bealls_list}. PredatoryJournals.org maintains
community-curated lists updated
monthly~\cite{predatoryjournals}. Regional authorities like the
Algerian Ministry of Higher Education publish lists of questionable
journals~\cite{DGRSDT2024}. The KSCIEN Organisation provides
specialized databases for predatory conferences, standalone journals,
publishers, and hijacked journals~\cite{kscien}. Retraction Watch
helps identify journals with problematic retraction
patterns~\cite{retraction_watch}. Optional Scopus coverage enables
checking against indexed journals. Additionally, the Custom List
Backend allows institutions and users to provide their own curated
lists in CSV or JSON format, supporting both predatory and legitimate
classifications to accommodate organization-specific policies and
regional requirements.

\textbf{Heuristic Pattern Analysis Backends} complement curated
databases by checking data quality indicators and metadata consistency
for journals not present in curated lists. These are not machine
learning models or predictive algorithms, but rather systematic checks
of observable patterns that suggest quality or concerns. The OpenAlex
Analyzer queries over 270,000 journals to examine publication volume
(e.g., $>$1000 papers/year suggests publication mill behavior), citation
ratios (e.g., $<$0.5 citations/paper indicates low impact), author
diversity, and growth patterns~\cite{openalex}. The Crossref Analyzer
checks metadata completeness and quality: presence of abstracts,
reference lists, author ORCID identifiers, funding information, and
licensing details~\cite{crossref}. Missing or incomplete metadata
suggests poor editorial standards. The Cross-Validator compares
information across sources to detect inconsistencies in publisher
names or journal metadata that may indicate fraudulent
activity. Pattern analysis provides supplemental evidence but carries
lower confidence than curated database matches.

\subsection{Assessment Methodology}

The tool employs a hybrid two-part approach with clearly differentiated
roles:

\textbf{Primary: Curated Database Lookups.} When a journal appears in
authoritative curated databases (DOAJ, Beall's List, Scopus, ministry
lists), the tool reports that determination with high confidence. These
databases represent expert human judgment and provide the most
reliable assessments. A match in DOAJ indicates legitimacy; a match in
Beall's List indicates a predatory publisher.

\textbf{Secondary: Heuristic Pattern Checks.} For journals absent from
curated databases, the tool examines observable patterns through
OpenAlex and Crossref: publication volumes, citation ratios, metadata
completeness, publisher consistency. These heuristic checks identify
potential warning signs (e.g., publication mills, missing metadata)
or positive indicators (e.g., complete metadata, healthy citation
patterns). Pattern analysis provides evidence-based suggestions, not
definitive judgments, and assessments carry lower confidence than
curated database matches. Pattern checks also supplement curated
database findings by providing additional context.

\textbf{Multi-Layered Defense Model.} The tool functions as one layer
in a comprehensive defense against predatory publishing. Curated
databases handle known predatory publishers identified through
retrospective analysis. Heuristic checks catch obvious quality
anomalies like publication mills or incomplete metadata. However, novel
predatory tactics that successfully mimic legitimate journal practices
will not be detected until they appear in curated databases or trigger
heuristic thresholds. For edge cases and sophisticated mimicry,
researchers must apply institutional guidelines, domain expertise, and
editorial board verification. The tool reduces manual verification
burden for routine cases while explicitly flagging uncertain
situations where human judgment remains essential.

\textbf{Aggregation as the Core Contribution.} The primary engineering
challenge lies in combining heterogeneous data sources. Each source
uses different naming conventions: DOAJ may list ``Journal of Computer
Science'' while Crossref records ``J. Comput. Sci.'' and OpenAlex
indexes the same journal under multiple name variants. ISSN formats
differ (``1234-5678'' vs ``12345678''), and publisher names vary
(``Springer'' vs ``Springer Nature'' vs ``Springer-Verlag''). Sources
update at different frequencies --- DOAJ quarterly, Beall's List
historically frozen, OpenAlex weekly --- creating temporal consistency
challenges. Different sources have different reliability profiles, and
they sometimes disagree on the same journal. Handling these
inconsistencies through robust normalization, fuzzy matching, and
conflict resolution is the real engineering contribution. The value
lies not in algorithmic sophistication but in systematically
aggregating scattered authoritative information into a unified,
queryable system.

Each assessment reports a confidence indicator as a transparency
mechanism. Confidence reflects the strength and consistency of
available evidence: higher when multiple independent sources agree on
the same classification, lower when evidence is sparse, sources
conflict, or only pattern analysis is available. This indicator helps
researchers understand the certainty of each assessment and make
informed decisions about whether additional verification is warranted.

\subsection{Transparency and Reproducibility}
A critical design principle of Aletheia-Probe is complete
transparency. The tool explicitly reports all data sources consulted
and their individual contributions to the final assessment. Each
result includes detailed reasoning showing which backends provided
evidence and the overall confidence indicator (see
Appendix~\ref{sec:examples} for representative output examples). This
transparency enables researchers to understand and validate the
assessment logic rather than accepting opaque "black box" results.

Furthermore, as open source software, all assessment algorithms are
publicly available and auditable. Researchers can examine the exact
logic used for pattern analysis, result aggregation, and confidence
indication. This reproducibility is essential for academic integrity
--- assessments can be independently verified, and the methodology can
be peer-reviewed and improved by the research community. The
combination of transparent output and open source implementation
ensures that journal assessments are not only automated but also
scientifically rigorous and verifiable.

\textbf{Temporal Dimension of Reproducibility.} While the tool's code
logic is fully reproducible, assessment results depend on the state of
underlying data sources at query time. A researcher assessing journal
``X'' today may receive a different classification in six months if
DOAJ updates their index, a ministry list adds new entries, or the
journal accumulates sufficient publication history to trigger pattern
analysis thresholds. This temporal variability is not a weakness but
reflects an important reality: journal legitimacy can genuinely change
over time. A journal may gain legitimacy through improved editorial
practices and DOAJ inclusion, or lose legitimacy if identified as
predatory. The tool's behavior is reproducible (same data state yields
same results), but data sources evolve to reflect the changing
scholarly publishing landscape. For research documentation, users
should record assessment timestamps and note that conclusions reflect
data state at the time of evaluation.

Importantly, Aletheia-Probe is a decision support tool, not a decision
maker. The tool presents assessment data and reasoning, but the final
judgment always rests with the human researcher. Users must evaluate
the evidence provided, consider their specific context, and make
informed decisions based on the tool's output combined with their own
expertise and institutional guidelines.

\subsection{Explicit UNKNOWN Results: Transparency Over Guessing}

A critical design feature of Aletheia-Probe is its explicit handling
of insufficient data. When the tool cannot make a reliable
determination, it returns an \texttt{UNKNOWN} or
\texttt{INSUFFICIENT\_DATA} classification rather than providing a
low-confidence guess. This intentional refusal to speculate is a
feature, not a limitation.

The global journal landscape contains millions of publications. Even
with extensive data aggregation covering over 240 million publication
records, curated databases have inherently limited coverage ---
e.g.~DOAJ indexes 22,000 journals, Beall's List approximately 2,900
entries, and Scopus only 30,000 journals. Many legitimate regional,
institutional, or newly established journals are not present in any
curated database and may lack sufficient publication history for
pattern analysis.

Rather than extrapolating from insufficient evidence, the tool
explicitly reports \texttt{UNKNOWN}. This transparency is more
valuable than providing unreliable assessments masked by low
confidence scores. An \texttt{UNKNOWN} result prompts researchers to
apply manual verification, consult institutional guidelines, or seek
domain expert assessment --- which is precisely the appropriate
response when automated methods cannot make reliable determinations.
The tool's explicit acknowledgment of uncertainty represents honest
transparency rather than algorithmic failure.

\section{Research Workflow Integration: A Systematic Review Example}

Beyond individual journal checks, Aletheia-Probe integrates into
research workflows requiring systematic quality assessment. We
illustrate this with a concrete systematic literature review scenario.

\textbf{Scenario:} A researcher conducts a systematic review on machine
learning applications in healthcare, following PRISMA
guidelines~\cite{page2021prisma}. Their initial search across Web of
Science, PubMed, and IEEE Xplore yields 847 potentially relevant
papers from 312 distinct journals. PRISMA requires explicit
documentation of source quality criteria, so they must verify each
journal's legitimacy before including papers in the analysis.

\textbf{Traditional Manual Workflow:} The researcher would manually
check each journal against multiple databases: (1) Search DOAJ website
for open access legitimacy; (2) Check Beall's List archives via
Internet Archive; (3) Search Scopus title lists (if institutional
access available); (4) Check ministry warning lists; (5) Manually
verify publisher legitimacy. For 150 unique journals, accounting for
name variations and cross-referencing across 5--10 sources,
many hours of tedious work. Inconsistent naming (``J.~
Comput.~Sci.'' vs ``Journal of Computer Science'') requires
careful manual matching. Results must be documented for PRISMA
compliance, typically in spreadsheets maintained manually.

\textbf{With Aletheia-Probe:} The researcher exports their bibliography
to BibTeX format and runs:
\begin{lstlisting}[language=bash,basicstyle=\ttfamily\scriptsize]
aletheia-probe bibtex references.bib --format json > results.json
\end{lstlisting}
The tool queries all configured backends concurrently, returning
results in typically under 10 minutes for 300 journals. Output
includes classifications, confidence indicators, and complete source
attribution. The researcher can post-process the results using
standard tools like texttt{jq} or import results into the review
management software, filters by confidence levels, and focuses manual
verification effort on \texttt{UNKNOWN} or low-confidence cases.  This
decreases the needed time for an evaluation dramatically --- but of
course depending on the coverage of the used journals, the researcher
still needs to process the results of the tool and possibly manually
evaluate \texttt{UNKNOWN} assessments.

This workflow transformation demonstrates the tool's practical value:
it doesn't eliminate human judgment but dramatically reduces the manual
burden for routine assessments while explicitly identifying cases where
human expertise is needed.

\section{Practical Features and Implementation}
The tool provides multiple interfaces for integration into existing
research workflows. Command-line queries enable assessment of
individual journals or conferences. Batch processing of BibTeX files
automatically checks entire bibliographies, with the current
implementation reducing assessment overhead. The tool supports
flexible output formats (human-readable text and JSON) for integration
with other tools. Configurable YAML-based settings enable
customization of backend selection and assessment thresholds. Data
caching in SQLite improves performance for repeated queries.

The tool is released as open source software under MIT license at
\url{https://github.com/sustainet-guardian/aletheia-probe}, enabling
community contributions and institutional customization.

\section{Conclusion}
This paper introduces Aletheia-Probe, a practical tool for assessing
journal legitimacy integrated into research workflows. The system
aggregates multiple authoritative data sources into a unified
assessment framework covering over 240 million publication records. The
hybrid methodology combines curated databases with pattern analysis,
achieving transparent evaluations through complete disclosure of
sources and reasoning. By transforming scattered quality indicators
into actionable guidance at the point of decision, the tool
demonstrates that the primary barrier to practical predatory journal
detection has been engineering infrastructure rather than algorithmic
novelty.

This initial presentation establishes the tool's architecture, design
philosophy, and practical integration approach. The modular design
invites community contributions to expand coverage and adapt
functionality to institutional needs. Future work will present
comprehensive empirical validation, including systematic coverage
analysis across large journal datasets, comparative evaluation against
existing approaches, and longitudinal studies of assessment accuracy.

\section{Acknowledgements}
This work was funded by the Federal Ministry of Research, Technology
and Space (BMFTR) in Germany under grant number 16KIS2251 of the
SUSTAINET-guardian project.

We gratefully acknowledge the organizations maintaining data sources:
DOAJ, Beall's List maintainers, OpenAlex, Crossref, Retraction Watch,
Scopus, and the Algerian Ministry of Higher Education.

The author declares no conflicts of interest.

\appendix

\section{Example Output}
\label{sec:examples}

These examples demonstrate the tool's core transparency principle:
every assessment explicitly reports all data sources consulted and
their individual contributions. The examples illustrate how the tool
presents complete evidence to researchers rather than providing opaque
scores.

\subsection{Straightforward Case: Well-Known Journal}
When curated databases agree, the tool reports high-confidence
assessments with clear source attribution:

\begin{lstlisting}[language=bash,basicstyle=\ttfamily\scriptsize]
$ aletheia-probe journal "Nature"
Journal: Nature
Assessment: LEGITIMATE
Confidence: 0.86
Overall Score: 0.76
Processing Time: 1.38s

Reasoning:
  * Classified as legitimate based on 4 source(s)
  * 155 retraction(s): 0.035% rate (within normal range
    for 446,231 publications)
  * doaj: legitimate (confidence: 0.70)
  * openalex_analyzer: legitimate (confidence: 0.64)
  * scopus: legitimate (confidence: 0.95)
  * Confidence boosted by agreement across multiple backends

Recommendation:
  ACCEPTABLE - Strong evidence of legitimacy, appears trustworthy
\end{lstlisting}

This straightforward case shows multiple independent sources (DOAJ,
OpenAlex, Scopus) providing converging evidence. The tool explicitly
lists all consulted sources and their individual assessments.

\subsection{Straightforward Case: Known Predatory Publisher}

Similarly, when curated predatory lists agree, the tool reports clear
assessments with source attribution:

\begin{lstlisting}[language=bash,basicstyle=\ttfamily\scriptsize]
$ aletheia-probe journal "2425 Publishers"
Journal: 2425 Publishers
Assessment: PREDATORY
Confidence: 1.00
Overall Score: 0.95
Processing Time: 0.79s

Reasoning:
  * Classified as predatory based on 2 predatory list(s)
  * bealls: predatory (confidence: 0.95)
  * predatoryjournals: predatory (confidence: 0.95)
  * Confidence boosted by agreement across multiple backends

Recommendation:
  AVOID - Strong evidence of predatory characteristics detected
\end{lstlisting}

Both straightforward examples demonstrate source transparency: every
assessment explicitly shows which databases were consulted and what
they found.

\subsection{Challenging Case: Conflicting Source Assessments}

The tool's transparency is most valuable when sources disagree. This
example demonstrates how the tool explicitly reports conflicting
evidence rather than hiding disagreement behind an opaque score:

\begin{lstlisting}[language=bash,basicstyle=\ttfamily\scriptsize]
$ aletheia-probe journal "Asian Journal of Chemistry"
Journal: Asian Journal of Chemistry
Assessment: PREDATORY
Confidence: 0.57
Overall Score: 0.57
Processing Time: 1.42s

Reasoning:
  * Classified as predatory based on 3 predatory list(s)
  * 1 retraction(s): 0.009% rate (within normal range
    for 11,116 publications)
  * algerian_ministry: predatory (confidence: 0.95)
  * bealls: predatory (confidence: 0.95)
  * openalex_analyzer: legitimate (confidence: 0.55)
  * predatoryjournals: predatory (confidence: 0.95)
  * scopus: legitimate (confidence: 0.95)
  * NOTE: Sources disagree (3 predatory, 3 legitimate) - review
    carefully

Recommendation:
  USE CAUTION - Some predatory indicators detected, investigate
  further
\end{lstlisting}

This example demonstrates the tool's core value: it explicitly reports
the conflict (``Sources disagree''), shows all contributing evidence,
and provides moderate confidence (0.57) reflecting genuine uncertainty.
Rather than forcing a high-confidence classification, the tool
transparently presents the contradictory evidence and flags the case
for manual review. This honest handling of ambiguity enables informed
human judgment.

\onecolumn
\phantomsection
\sloppy
\Urlmuskip=0mu plus 1mu\relax
\printbibliography

\end{document}
