\documentclass[fleqn,10pt]{SelfArx}

\usepackage[english]{babel}
\usepackage{listings}
\usepackage{xcolor}
%\usepackage{siunitx}
\usepackage{orcidlink}

\frenchspacing

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
  backgroundcolor=\color{backcolour},
  commentstyle=\color{codegreen},
  keywordstyle=\color{magenta},
  numberstyle=\tiny\color{codegray},
  stringstyle=\color{codepurple},
  basicstyle=\ttfamily\footnotesize,
  breakatwhitespace=false,
  breaklines=true,
  captionpos=b,
  keepspaces=true,
  numbers=left,
  numbersep=5pt,
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2
}

\lstset{style=mystyle}

\usepackage[style=numeric,sorting=none,backend=biber]{biblatex}
\addbibresource{AletheiaProbe-short.bib}
\usepackage{csquotes}
\usepackage{enumitem}

\setlength{\headheight}{20.96901pt}
\setlength{\columnsep}{0.55cm}
\setlength{\fboxrule}{0.75pt}

\definecolor{color1}{RGB}{0,0,90}
\definecolor{color2}{RGB}{0,20,20}

\newcommand{\ilcode}[1]{%
  \texttt{\small\textcolor{darkgray}{#1}}%
}

\usepackage{hyperref}
\hypersetup{
  hidelinks,
  colorlinks,
  breaklinks=true,
  urlcolor=color2,
  citecolor=color1,
  linkcolor=color1,
  bookmarksopen=false,
  pdftitle={Aletheia-Probe: Automated Journal Quality Assessment},
  pdfauthor={Andreas Florath}
}

\JournalInfo{V1}
\Archive{}
\PaperTitle{Aletheia-Probe: A Multi-Source Tool for Automated Journal Assessment}
\Authors{Andreas Florath\orcidlink{0009-0001-6471-7372}\textsuperscript{1}}
\affiliation{\textsuperscript{1}\textit{Deutsche Telekom AG, Andreas.Florath@telekom.de}}
\Keywords{Predatory Journals --- Academic Integrity --- Publication Ethics --- Journal Assessment --- Research Workflows}
\newcommand{\keywordname}{Keywords}

\Abstract{Researchers need to assess journal legitimacy when
  conducting literature reviews, selecting publication venues, and
  verifying citations, but authoritative information is scattered
  across multiple incompatible databases. This paper introduces
  Aletheia-Probe, an open-source tool that systematically aggregates
  curated databases and pattern analysis from multiple authoritative
  sources to provide transparent, confidence-scored journal
  assessments. Rather than providing opaque recommendations, the tool
  explicitly reports which sources were consulted, what each found, and
  where evidence conflicts, enabling researchers to make informed
  decisions. Designed for integration into research workflows through
  command-line and programmatic interfaces, the tool reduces manual
  assessment overhead from hours to minutes while explicitly flagging
  uncertain cases requiring human expertise. We present the tool's
  architecture, core design principles, and practical integration
  approach. Comprehensive empirical validation will be presented in
  forthcoming work.}

\begin{document}

\maketitle

\tableofcontents 

\section{Introduction}
Predatory academic journals pose a significant and growing threat to
scholarly integrity. Article volumes in predatory journals grew from
53,000 in 2010 to over 420,000 in 2014~\cite{shen2015predatory},
affecting researchers across all scientific disciplines---from
mathematics and engineering to nursing and biomedical
sciences~\cite{agricola2025,gabrielsson2020nursing}. These journals
increasingly mimic legitimate publications through
professional-looking websites and editorial structures, making them
difficult to distinguish when researchers choose publication venues or
conduct literature reviews.

Multiple organizations maintain lists of legitimate or predatory
journals --- Beall's List~\cite{bealls_list}, the Directory of Open
Access Journals (DOAJ)~\cite{doaj}, indexing services like
Scopus~\cite{scopus}, and regional ministry
lists~\cite{DGRSDT2024}. However, researchers lack practical tools to
use these scattered resources. Who has time to manually check hundreds
or even thousands of journal entries across multiple databases and
websites? A literature review with 150 papers requires verifying each
source journal, which conservatively amounts to 12 hours of tedious
manual work across multiple websites.

This paper introduces Aletheia-Probe~\cite{aletheia_probe}, an
automated journal assessment tool that combines data from multiple
trusted sources with transparent confidence indicators. ``Aletheia''
comes from ancient Greek philosophy, representing truth and
unconcealment --- reflecting the tool's mission to reveal the truth
about academic journals. By reducing assessment overhead from hours to
minutes, the tool transforms scattered quality indicators into
actionable guidance at the point of decision.

This tool functions like a virus scanner for academic publishing---
designed to be installed in everyday research work but hopefully
triggering alerts only for edge cases. Honest researchers using
established, well-known journals through major search engines will
likely never encounter predatory venues; the tool catches less obvious
threats that might slip through normal workflows. This paper presents
Aletheia-Probe's architecture, design principles, and practical
integration approach. The system aggregates multiple authoritative
data sources to provide assessments. Comprehensive empirical
validation, including systematic coverage analysis and comparative
evaluation, will be presented in forthcoming work.

\section{Problem and Motivation}

\textbf{Concrete Researcher Scenarios.} Consider a systematic review
author conducting a meta-analysis who must verify 300 journals for
PRISMA compliance~\cite{page2021prisma}. PRISMA requires explicit
documentation of source quality criteria. DOAJ alone doesn't cover
many regional journals, so the researcher must manually check each
journal against multiple databases --- DOAJ for open access journals,
Scopus for indexed venues, Beall's List archives for known predatory
publishers, ministry lists for regional warnings. Cross-referencing
300 journals across 5--10 sources, accounting for name variations and
ISSN formats, conservatively requires 12 hours of tedious, error-prone
manual work.

Or consider an early-career researcher seeking to publish in an
open-access journal. They receive invitations from dozens of venues
with professional-looking websites, impressive editorial boards, and
promises of fast peer review. Without institutional guidance, how do
they distinguish legitimate journals from predatory ones? Manually
checking each venue against multiple databases is time-consuming, and
missing a predatory journal can damage career prospects.

Similarly, research integrity officers investigating suspicious
citations must verify journal legitimacy across institutional
publications. Librarians managing budgets need to assess whether
open-access journals merit institutional support. In all these
scenarios, researchers need quick, reliable assessments but face
scattered information across incompatible sources.

\textbf{Compounding Factors.} The challenge intensifies due to several
factors. First, journal proliferation has accelerated with open access
publishing, creating a larger landscape to evaluate. Second, while
predatory journals do evolve tactics to mimic legitimate publications,
no automated tool can detect all novel approaches --- curated lists
are retrospective, and heuristic checks catch obvious anomalies but
not sophisticated mimicry. Assessment tools form one layer of defense
alongside institutional guidelines and domain expertise. Third,
effective assessment requires checking multiple independent sources,
as no single database provides comprehensive coverage. Finally,
researchers need answers in seconds, not hours --- publication venue
decisions cannot wait for days of manual verification.

Previous work has extensively studied predatory publishing. Bohannon's
sting operation submitted flawed papers to 304 open-access journals
and found over half accepted them with minimal peer
review~\cite{bohannon2013garbage}. Agricola et al. analyzed predatory
journal characteristics and provided practical
recommendations~\cite{agricola2025}. Grudniewicz et al. worked with
experts to standardize the definition of ``predatory
journal''~\cite{grudniewicz2019predatory}. While machine learning
approaches like AJPC have been explored~\cite{chen2023ajpc}, no
existing tool systematically aggregates authoritative curated sources
with transparent confidence indicators.

\section{System Design and Features}

\subsection{Core Design Principles}

Aletheia-Probe's architecture follows three fundamental principles that
shape all design decisions and distinguish it from other approaches:

\textbf{1. Aggregator, Not Creator.} The tool is a data aggregator that
systematically combines existing authoritative sources rather than
creating its own judgments. It acknowledges that expert curation is
distributed across the scholarly community---DOAJ for open access,
Beall's for predatory publishers, Scopus for indexed journals. The
tool's value lies in making this scattered expertise accessible and
queryable through a unified interface.

\textbf{2. Multiple Independent Sources.} The tool queries multiple
data sources for each assessment and reports all findings. No single
database provides comprehensive coverage or perfect accuracy. DOAJ
covers legitimate open access journals but not subscription venues.
Beall's List captures historically identified predatory publishers but
is no longer updated. Scopus indexes established journals but has
limited regional coverage. By consulting multiple independent sources,
the tool provides more complete information than any single database
and enables researchers to see when sources agree or conflict.

\textbf{3. Transparent Reasoning.} Every assessment explicitly reports
which sources were consulted, what each source found, and how that
evidence contributed to the overall classification. This transparency
is not a limitation but a core feature. Rather than providing opaque
``trust this score'' outputs, the tool shows its work, enabling
researchers to understand the basis for each assessment and make
informed decisions. When data is insufficient, the tool says so
explicitly (returning \texttt{UNKNOWN}) rather than manufacturing
low-confidence guesses. This honest reporting of uncertainty respects
researchers' need to understand the reliability of information they act
upon.

These principles prioritize transparency, expert curation, and
comprehensive information over algorithmic sophistication. The
following subsections describe how these principles manifest in system
architecture and functionality.

\subsection{Architecture Overview}
The system comprises five main components. The data synchronization
layer downloads information from multiple data-sets during initial
setup, organizing data for fast searching. The backend abstraction
layer provides a common interface to query all sources, whether local
databases or web APIs. The assessment dispatcher sends queries to all
relevant backends simultaneously, improving speed. The aggregation
component combines responses from multiple sources into a unified
assessment, reporting a confidence indicator alongside the
classification. Finally, command-line and library interfaces provide
two usage modes.

\subsection{Data Sources}

The tool integrates multiple data sources organized into two
categories:

\textbf{Curated Database Backends} provide authoritative yes/no
determinations for journals they cover through expert human curation.
The Directory of Open Access Journals (DOAJ) maintains a curated list
of over 22,000 legitimate open access journals~\cite{doaj}. Beall's
List, while no longer actively maintained, provides historical
archives of approximately 2,900 predatory publishers and
journals~\cite{bealls_list}. PredatoryJournals.org maintains
community-curated lists updated
monthly~\cite{predatoryjournals}. Regional authorities like the
Algerian Ministry of Higher Education publish lists of questionable
journals~\cite{DGRSDT2024}. The KSCIEN Organisation provides
specialized databases for predatory conferences, standalone journals,
publishers, and hijacked journals~\cite{kscien}. Retraction Watch
helps identify journals with problematic retraction
patterns~\cite{retraction_watch}. Optional Scopus coverage enables
checking against indexed journals. Additionally, the Custom List
Backend allows institutions and users to provide their own curated
lists in CSV or JSON format, supporting both predatory and legitimate
classifications to accommodate organization-specific policies and
regional requirements.

\textbf{Heuristic Pattern Analysis Backends} complement curated
databases by checking data quality indicators and metadata consistency
for journals not present in curated lists. These are not machine
learning models or predictive algorithms, but rather systematic checks
of observable patterns that suggest quality or concerns. The OpenAlex
Analyzer queries over 270,000 journals to examine publication volume
(e.g., $>$1000 papers/year suggests publication mill behavior), citation
ratios (e.g., $<$0.5 citations/paper indicates low impact), author
diversity, and growth patterns~\cite{openalex}. The Crossref Analyzer
checks metadata completeness and quality: presence of abstracts,
reference lists, author ORCID identifiers, funding information, and
licensing details~\cite{crossref}. Missing or incomplete metadata
suggests poor editorial standards. The Cross-Validator compares
information across sources to detect inconsistencies in publisher
names or journal metadata that may indicate fraudulent
activity. Pattern analysis provides supplemental evidence but carries
lower confidence than curated database matches.

\subsection{Geographic and Disciplinary Coverage Limitations}

The tool's coverage reflects inherent biases in its underlying data
sources. DOAJ, Beall's List, and Scopus have strong English-language
and Western institutional focus. Researchers from Sub-Saharan Africa,
Southeast Asia, Latin America, or Eastern Europe will encounter more
\texttt{UNKNOWN} results for legitimate regional journals in their
local languages. Similarly, coverage varies by discipline: biomedical
and physical sciences have better representation than social sciences,
humanities, or regional studies.

This limitation is not unique to Aletheia-Probe but reflects the
global scholarly publishing landscape's structural inequities. We
acknowledge this upfront: the tool currently serves researchers
working with internationally indexed, English-language journals more
effectively than those working with regional, non-English publications.
The Custom List Backend partially addresses this by allowing
institutions to supplement with regional databases, but comprehensive
global coverage remains an open challenge for the entire scholarly
assessment ecosystem.

Researchers using the tool should interpret \texttt{UNKNOWN} results in
this context. An \texttt{UNKNOWN} assessment for a regional journal
published in Portuguese, Arabic, or Bahasa Indonesia likely reflects
data source gaps rather than journal quality. The tool's transparency
about these limitations enables researchers to apply appropriate
judgment based on their geographic and disciplinary context.

\subsection{Assessment Methodology}

The tool employs a hybrid two-part approach with clearly differentiated
roles:

\textbf{Primary: Curated Database Lookups.} When a journal appears in
authoritative curated databases (DOAJ, Beall's List, Scopus, ministry
lists), the tool reports that determination with high confidence. These
databases represent expert human judgment and provide the most
reliable assessments. A match in DOAJ indicates legitimacy; a match in
Beall's List indicates a predatory publisher.

\textbf{Secondary: Heuristic Pattern Checks.} For journals absent from
curated databases, the tool examines observable patterns through
OpenAlex and Crossref: publication volumes, citation ratios, metadata
completeness, publisher consistency. These heuristic checks identify
potential warning signs (e.g., publication mills, missing metadata)
or positive indicators (e.g., complete metadata, healthy citation
patterns). Pattern analysis provides evidence-based suggestions, not
definitive judgments, and assessments carry lower confidence than
curated database matches. Pattern checks also supplement curated
database findings by providing additional context. However,
sophisticated predatory tactics that successfully mimic legitimate
journals will not be detected until they appear in curated databases or
their anomalies trigger pattern analysis. For such edge cases,
researchers must apply institutional guidelines, domain expertise, and
editorial board verification.

\textbf{Aggregation as the Core Contribution.} The primary engineering
challenge lies in combining heterogeneous data sources. Each source
uses different naming conventions: DOAJ may list ``Journal of Computer
Science'' while Crossref records ``J. Comput. Sci.'' and OpenAlex
indexes the same journal under multiple name variants. ISSN formats
differ (``1234-5678'' vs ``12345678''), and publisher names vary
(``Springer'' vs ``Springer Nature'' vs ``Springer-Verlag''). Sources
update at different frequencies --- DOAJ quarterly, Beall's List
historically frozen, OpenAlex weekly --- creating temporal consistency
challenges. Different sources have different reliability profiles, and
they sometimes disagree on the same journal. Handling these
inconsistencies through robust normalization, fuzzy matching, and
conflict resolution is the real engineering contribution. The value
lies not in algorithmic sophistication but in systematically
aggregating scattered authoritative information into a unified,
queryable system.

Each assessment reports a confidence indicator as a transparency
mechanism. Confidence reflects the strength and consistency of
available evidence: higher when multiple independent sources agree on
the same classification, lower when evidence is sparse, sources
conflict, or only pattern analysis is available. This indicator helps
researchers understand the certainty of each assessment and make
informed decisions about whether additional verification is warranted.

\subsection{Transparency and Reproducibility}
\label{sec:transparency}

Transparency manifests in four complementary ways throughout the tool's
design and operation.

\subsubsection{Transparent Reasoning and Source Attribution}

The tool explicitly reports all data sources consulted and their
individual contributions to each assessment. Every result includes
detailed reasoning showing which backends provided evidence, their
confidence indicators, and where sources agree or conflict (see
Appendix~\ref{sec:examples}). Rather than providing opaque ``trust
this score'' outputs, the tool presents all available evidence---what
each source found, how confident each assessment is---enabling
researchers to understand the basis for classification and make
informed decisions according to their institutional policies and risk
tolerance. This transparency enables validation rather than requiring
blind trust.

\subsubsection{Open Source Code Auditability}

All assessment algorithms are publicly available and auditable.
Researchers can examine the exact logic used for pattern analysis,
result aggregation, and confidence indication. This reproducibility is
essential for academic integrity---assessments can be independently
verified, and the methodology can be peer-reviewed and improved by the
research community. The combination of transparent output and open
source implementation ensures scientifically rigorous and verifiable
journal assessments.

\subsubsection{Temporal Dimension of Reproducibility}

While code logic is fully reproducible, assessment results depend on
data source state at query time. A journal assessed today may receive a
different classification in six months if DOAJ updates their index,
ministry lists change, or publication history accumulates. This
temporal variability reflects reality: journal legitimacy genuinely
changes over time. A journal may gain legitimacy through DOAJ inclusion
or lose it if identified as predatory. The tool's behavior is
reproducible (same data state yields same results), but data sources
evolve. For research documentation, users should record assessment
timestamps noting conclusions reflect data state at evaluation time.

\subsubsection{Explicit UNKNOWN Results as Honest Transparency}

When the tool cannot make a reliable determination, it returns
\texttt{UNKNOWN} or \texttt{INSUFFICIENT\_DATA} rather than
low-confidence guesses. This intentional refusal to speculate is a
feature, not a limitation. Curated databases have inherently limited
coverage (DOAJ: 22,000 journals; Beall's: 2,900 entries; Scopus:
30,000). Many legitimate regional, institutional, or newly established
journals lack coverage and publication history for pattern analysis.
Rather than extrapolating from insufficient evidence, the tool
explicitly reports \texttt{UNKNOWN}---more valuable than unreliable
assessments masked by low confidence scores. \texttt{UNKNOWN} results
prompt researchers to apply manual verification, consult institutional
guidelines, or seek expert assessment, which is precisely the
appropriate response when automated methods cannot make reliable
determinations. This explicit acknowledgment of uncertainty represents
honest transparency rather than algorithmic failure.

\section{Research Workflow Integration: A Systematic Review Example}

Beyond individual journal checks, Aletheia-Probe integrates into
research workflows requiring systematic quality assessment. We
illustrate this with a concrete systematic literature review scenario.

\textbf{Scenario:} A researcher conducts a systematic review on machine
learning applications in healthcare, following PRISMA
guidelines~\cite{page2021prisma}. Their initial search across Web of
Science, PubMed, and IEEE Xplore yields 847 potentially relevant
papers from 312 distinct journals. PRISMA requires explicit
documentation of source quality criteria, so they must verify each
journal's legitimacy before including papers in the analysis.

\textbf{Traditional Manual Workflow:} The researcher would manually
check each journal against multiple databases (DOAJ, Beall's List,
Scopus, ministry lists), accounting for naming variations and
cross-referencing across sources. For 150 unique journals, this process
requires many hours of tedious, error-prone manual work.

\textbf{With Aletheia-Probe:} The researcher exports their bibliography
to BibTeX format and runs:
\begin{lstlisting}[language=bash,basicstyle=\ttfamily\scriptsize]
aletheia-probe bibtex references.bib --format json > results.json
\end{lstlisting}
The tool queries backends concurrently, typically returning results in
under 10 minutes for 300 journals. Rather than eliminating human
judgment, this dramatically reduces manual burden for routine cases
while explicitly flagging cases needing expert verification. The
researcher can post-process results using standard tools or import into
review management software, filtering by confidence levels and focusing
manual verification effort on \texttt{UNKNOWN} or low-confidence cases.

This workflow transformation demonstrates the tool's practical value:
it doesn't eliminate human judgment but dramatically reduces the manual
burden for routine assessments while explicitly identifying cases where
human expertise is needed.

\section{Practical Features and Implementation}

The tool's design directly supports diverse researcher workflows
through targeted features:

\textbf{Quick Verification Checks.} Researchers encountering an
unfamiliar journal invitation or citation can quickly verify legitimacy
through command-line queries. The \ilcode{aletheia-probe journal
"Journal Name"} command returns human-readable assessments in seconds,
showing all consulted sources and their findings. This serves
early-career researchers evaluating publication invitations, reviewers
checking citation quality, or research integrity officers investigating
specific journals.

\textbf{Systematic Literature Reviews.} Batch processing of BibTeX
files enables efficient verification of entire bibliographies. The
\ilcode{aletheia-probe bibtex references.bib} command processes
hundreds of journals concurrently, automating what would otherwise
require hours of manual cross-referencing across multiple databases.
This directly supports PRISMA-compliant systematic reviews requiring
explicit source quality documentation.

\textbf{Integration with Research Infrastructure.} JSON output format
(\ilcode{--format json}) enables integration into institutional
workflows, bibliography management systems, and research data
pipelines. Institutions can incorporate journal assessment into
existing submission workflows, research information systems, or
automated compliance checking tools.

\textbf{Institutional Customization.} YAML-based configuration allows
institutions to customize backend selection, assessment thresholds, and
add custom journal lists reflecting institutional policies. Research
libraries can configure the tool to match local acquisition criteria,
while regional institutions can supplement with local language journal
databases.

\textbf{Performance Optimization.} SQLite caching stores previous
assessments and data source queries, improving performance for repeated
queries. This benefits research groups working on related projects or
institutions processing multiple systematic reviews concurrently.

The tool is released as open source software under MIT license at
\url{https://github.com/sustainet-guardian/aletheia-probe}, enabling
community contributions and institutional customization.

\section{Appropriate Use: When to Use the Tool vs. Manual Verification}
\label{sec:appropriate-use}

Understanding the tool's intended scope helps researchers apply it
effectively as one component of their assessment toolkit.

\textbf{Appropriate Use Cases.} The tool is designed for everyday
journal assessment during literature reviews, publication venue
selection, and routine verification checks. Use it when:
\begin{itemize}[noitemsep,topsep=0pt]
\item Conducting systematic literature reviews requiring source quality
  documentation
\item Evaluating unfamiliar journals encountered during citation
  searching
\item Screening publication venue invitations
\item Performing routine institutional compliance checking
\item Processing bibliographies for research integrity audits
\end{itemize}

For these routine scenarios, the tool provides efficient, transparent
assessment that dramatically reduces manual verification burden while
explicitly flagging uncertain cases.

\textbf{When Additional Verification Is Warranted.} For high-stakes or
edge-case decisions, researchers should supplement the tool's output
with institutional guidelines and domain expert consultation:
\begin{itemize}[noitemsep,topsep=0pt]
\item Publication venue selection affecting tenure or promotion
  decisions
\item Journals receiving \texttt{UNKNOWN} or low-confidence assessments
\item Regional journals in languages or disciplines with limited data
  source coverage
\item Recently established journals lacking sufficient publication
  history
\item Cases where institutional policies impose stricter criteria than
  data source standards
\end{itemize}

In these situations, the tool provides valuable preliminary assessment
and transparent evidence presentation, but researchers should verify
findings against institutional guidelines, consult editorial board
composition, review sample published articles, or seek domain expert
input.

\textbf{Tool as One Component.} Aletheia-Probe functions as one layer
in a comprehensive approach to journal quality assessment. It excels at
efficiently aggregating scattered authoritative information and
explicitly reporting uncertainty, but it does not replace institutional
policies, domain expertise, or professional judgment. This appropriate
scope---automating routine checks while flagging cases requiring human
expertise---makes the tool a trustworthy component of the researcher's
toolkit rather than claiming to be the sole arbiter of journal
legitimacy.

\section{Limitations}

\textbf{No Predictive Capability.} The tool assesses current journal
status based on existing evidence; it does not predict whether a
journal will become predatory in the future or whether a currently
legitimate journal will maintain standards. A journal currently
classified as legitimate may later be identified as predatory if
editorial practices deteriorate.

\textbf{Legitimacy, Not Research Quality.} The tool assesses journal
legitimacy (whether the venue is predatory), not research quality or
peer review rigor. A legitimate journal may publish poor-quality
research, and a predatory journal may occasionally publish valid work.
The tool does not evaluate individual article quality, methodological
soundness, or the strength of peer review processes.

\textbf{Limited Handling of Edge Cases.} The tool may not adequately
detect sophisticated fraud scenarios like hijacked journals (legitimate
journals whose websites are cloned by predatory actors), temporarily
compromised journal accounts, or journals that recently changed
ownership. The Kscien Hijacked Journals database provides some
coverage, but novel hijacking attempts may not be detected until
reported and added to databases.

As discussed in Section~\ref{sec:transparency} and
Section~\ref{sec:appropriate-use}, assessments reflect data source state
at query time and the tool functions as one component alongside
institutional policies and expert judgment, not as their substitute.

\section{Conclusion}
This paper introduces Aletheia-Probe, a practical tool for assessing
journal legitimacy integrated into research workflows. The system
aggregates multiple authoritative data sources into a unified
assessment framework. The hybrid methodology combines curated databases
with pattern analysis, achieving transparent evaluations through
complete disclosure of sources and reasoning. By transforming scattered quality indicators
into actionable guidance at the point of decision, the tool
demonstrates that the primary barrier to practical predatory journal
detection has been engineering infrastructure rather than algorithmic
novelty.

This initial presentation establishes the tool's architecture, design
philosophy, and practical integration approach. The modular design
invites community contributions to expand coverage and adapt
functionality to institutional needs. Future work will present
comprehensive empirical validation, including systematic coverage
analysis across large journal datasets, comparative evaluation against
existing approaches, and longitudinal studies of assessment accuracy.

\section{Acknowledgements}
This work was funded by the Federal Ministry of Research, Technology
and Space (BMFTR) in Germany under grant number 16KIS2251 of the
SUSTAINET-guardian project.

We gratefully acknowledge the organizations maintaining data sources:
DOAJ, Beall's List maintainers, OpenAlex, Crossref, Retraction Watch,
Scopus, and the Algerian Ministry of Higher Education.

The author declares no conflicts of interest.

\appendix

\section{Example Output}
\label{sec:examples}

The examples below demonstrate the tool's core transparency principle at
different confidence levels. Subsections A.1 and A.2 illustrate
straightforward cases where multiple sources agree, enabling
high-confidence assessments and showing the tool's standard operation
when evidence is clear. Subsection A.3 demonstrates the tool's most
important function: explicitly presenting conflicting evidence when
sources disagree, enabling researchers to make informed judgments rather
than accepting opaque scores. This transparent handling of
ambiguity---showing \emph{all} source disagreements rather than
obscuring them behind averaged scores---distinguishes the tool's
approach from black-box alternatives.

\subsection{Straightforward Case: Well-Known Journal}
When curated databases agree, the tool reports high-confidence
assessments with clear source attribution:

\begin{lstlisting}[language=bash,basicstyle=\ttfamily\scriptsize]
$ aletheia-probe journal "Nature"
Journal: Nature
Assessment: LEGITIMATE
Confidence: 0.86
Overall Score: 0.76
Processing Time: 1.38s

Reasoning:
  * Classified as legitimate based on 4 source(s)
  * 155 retraction(s): 0.035% rate (within normal range
    for 446,231 publications)
  * doaj: legitimate (confidence: 0.70)
  * openalex_analyzer: legitimate (confidence: 0.64)
  * scopus: legitimate (confidence: 0.95)
  * Confidence boosted by agreement across multiple backends

Recommendation:
  ACCEPTABLE - Strong evidence of legitimacy, appears trustworthy
\end{lstlisting}

This straightforward case shows multiple independent sources (DOAJ,
OpenAlex, Scopus) providing converging evidence. The tool explicitly
lists all consulted sources and their individual assessments.

\subsection{Straightforward Case: Known Predatory Publisher}

Similarly, when curated predatory lists agree, the tool reports clear
assessments with source attribution:

\begin{lstlisting}[language=bash,basicstyle=\ttfamily\scriptsize]
$ aletheia-probe journal "2425 Publishers"
Journal: 2425 Publishers
Assessment: PREDATORY
Confidence: 1.00
Overall Score: 0.95
Processing Time: 0.79s

Reasoning:
  * Classified as predatory based on 2 predatory list(s)
  * bealls: predatory (confidence: 0.95)
  * predatoryjournals: predatory (confidence: 0.95)
  * Confidence boosted by agreement across multiple backends

Recommendation:
  AVOID - Strong evidence of predatory characteristics detected
\end{lstlisting}

Both straightforward examples demonstrate source transparency: every
assessment explicitly shows which databases were consulted and what
they found.

\subsection{Challenging Case: Conflicting Source Assessments}

The tool's transparency is most valuable when sources disagree. This
example demonstrates how the tool explicitly reports conflicting
evidence rather than hiding disagreement behind an opaque score:

\begin{lstlisting}[language=bash,basicstyle=\ttfamily\scriptsize]
$ aletheia-probe journal "Asian Journal of Chemistry"
Journal: Asian Journal of Chemistry
Assessment: PREDATORY
Confidence: 0.57
Overall Score: 0.57
Processing Time: 1.42s

Reasoning:
  * Classified as predatory based on 3 predatory list(s)
  * 1 retraction(s): 0.009% rate (within normal range
    for 11,116 publications)
  * algerian_ministry: predatory (confidence: 0.95)
  * bealls: predatory (confidence: 0.95)
  * openalex_analyzer: legitimate (confidence: 0.55)
  * predatoryjournals: predatory (confidence: 0.95)
  * scopus: legitimate (confidence: 0.95)
  * NOTE: Sources disagree (3 predatory, 3 legitimate) - review
    carefully

Recommendation:
  USE CAUTION - Some predatory indicators detected, investigate
  further
\end{lstlisting}

This example demonstrates the tool's core value: it explicitly reports
the conflict (``Sources disagree''), shows all contributing evidence,
and provides moderate confidence (0.57) reflecting genuine uncertainty.
Rather than forcing a high-confidence classification, the tool
transparently presents the contradictory evidence and flags the case
for manual review. This honest handling of ambiguity enables informed
human judgment.

\onecolumn
\phantomsection
\sloppy
\Urlmuskip=0mu plus 1mu\relax
\printbibliography

\end{document}
